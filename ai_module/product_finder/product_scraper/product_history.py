"""
ìƒí’ˆ íˆìŠ¤í† ë¦¬ ê´€ë¦¬ (CSV ê¸°ë°˜)
ì´ë¯¸ ì‘ì„±í•œ ìƒí’ˆ URL ì¶”ì 
"""

from __future__ import annotations

import csv
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Set
from dataclasses import dataclass
from urllib.parse import urlparse, urlunparse


logger = logging.getLogger(__name__)


def normalize_url(url: str) -> str:
    """
    URL ì •ê·œí™” (ì¤‘ë³µ ì²´í¬ìš©)
    - Fragment ì œê±°
    - ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ëŠ” ë³´ì¡´ (ìƒí’ˆ ì‹ë³„ì ìœ ì§€: goodsNo, goodscode ë“±)
    - ì†Œë¬¸ì ë³€í™˜
    - trailing slash ì œê±°
    
    ì˜ˆ:
        https://www.musinsa.com/products/123?ref=search#reviews
        -> https://www.musinsa.com/products/123?ref=search
    """
    if not url:
        return ""

    try:
        parsed = urlparse(url.strip())

        # scheme, netloc, path + query ì‚¬ìš© (fragment ì œê±°)
        normalized = urlunparse((
            parsed.scheme.lower(),
            parsed.netloc.lower(),
            parsed.path.rstrip('/'),  # trailing slash ì œê±°
            '',  # params
            parsed.query,  # query ë³´ì¡´ (ìƒí’ˆ ì‹ë³„ì ë³´ì¡´)
            ''   # fragment ì œê±°
        ))

        return normalized

    except Exception as e:
        logger.warning(f"URL ì •ê·œí™” ì‹¤íŒ¨: {url} - {e}")
        return url.strip().lower()


@dataclass
class ProductRecord:
    """ìƒí’ˆ ê¸°ë¡"""
    product_url: str
    keyword: Optional[str] = None
    created_at: Optional[str] = None
    product_name: Optional[str] = None

    def to_dict(self):
        return {
            "product_url": self.product_url,
            "keyword": self.keyword or "",
            "created_at": self.created_at or "",
            "product_name": self.product_name or "",
        }


class ProductHistory:
    """ìƒí’ˆ íˆìŠ¤í† ë¦¬ ê´€ë¦¬"""

    FIELDNAMES = ['product_url', 'keyword', 'created_at', 'product_name']

    def __init__(self, csv_path: str | Path = "product_list.csv"):
        """
        Args:
            csv_path: CSV íŒŒì¼ ê²½ë¡œ
        """
        self.csv_path = Path(csv_path)
        self.logger = logging.getLogger(self.__class__.__name__)

        # CSV íŒŒì¼ ì´ˆê¸°í™”
        self._ensure_csv_exists()

    def _ensure_csv_exists(self):
        """CSV íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒì„±í•˜ê³ , ìˆìœ¼ë©´ í—¤ë” í™•ì¸"""
        if not self.csv_path.exists():
            self.logger.info(f"ğŸ“„ CSV íŒŒì¼ ìƒì„±: {self.csv_path}")
            self.csv_path.parent.mkdir(parents=True, exist_ok=True)
            self._write_header()
        else:
            # ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ í—¤ë” í™•ì¸
            self._validate_and_fix_header()

    def _write_header(self):
        """í—¤ë” ì‘ì„±"""
        with open(self.csv_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=self.FIELDNAMES)
            writer.writeheader()

    def _validate_and_fix_header(self):
        """
        ê¸°ì¡´ CSV íŒŒì¼ì˜ í—¤ë” ê²€ì¦ ë° ìˆ˜ì •
        í—¤ë”ê°€ ì—†ê±°ë‚˜ ì˜ëª»ë˜ì–´ ìˆìœ¼ë©´ ìˆ˜ì •
        """
        try:
            with open(self.csv_path, 'r', encoding='utf-8') as f:
                first_line = f.readline().strip()

            # í—¤ë”ê°€ ì—†ê±°ë‚˜ ì˜ëª»ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            expected_header = ','.join(self.FIELDNAMES)

            if not first_line or first_line != expected_header:
                self.logger.warning("âš ï¸  CSV í—¤ë”ê°€ ì—†ê±°ë‚˜ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. ìˆ˜ì • ì¤‘...")
                self._fix_csv_header()
            else:
                self.logger.debug("âœ… CSV í—¤ë” ì •ìƒ")

        except Exception as e:
            self.logger.error(f"í—¤ë” ê²€ì¦ ì‹¤íŒ¨: {e}")

    def _fix_csv_header(self):
        """í—¤ë”ê°€ ì—†ëŠ” CSV íŒŒì¼ì— í—¤ë” ì¶”ê°€"""
        try:
            # ê¸°ì¡´ ë°ì´í„° ì½ê¸°
            existing_data = []
            with open(self.csv_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and line.startswith('http'):  # URLë¡œ ì‹œì‘í•˜ëŠ” ì¤„ë§Œ
                        existing_data.append(line)

            # í—¤ë”ì™€ í•¨ê»˜ ë‹¤ì‹œ ì“°ê¸°
            with open(self.csv_path, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=self.FIELDNAMES)
                writer.writeheader()

                # ê¸°ì¡´ ë°ì´í„° ë³µì›
                for line in existing_data:
                    parts = line.split(',')
                    if len(parts) >= 1:
                        writer.writerow({
                            'product_url': parts[0] if len(parts) > 0 else '',
                            'keyword': parts[1] if len(parts) > 1 else '',
                            'created_at': parts[2] if len(parts) > 2 else '',
                            'product_name': parts[3] if len(parts) > 3 else '',
                        })

            self.logger.info("âœ… CSV í—¤ë” ì¶”ê°€ ì™„ë£Œ")

        except Exception as e:
            self.logger.error(f"CSV í—¤ë” ìˆ˜ì • ì‹¤íŒ¨: {e}")

    def get_all_urls(self) -> Set[str]:
        """
        ì €ì¥ëœ ëª¨ë“  ìƒí’ˆ URL ë°˜í™˜ (ì •ê·œí™”ëœ í˜•íƒœ)
        
        Returns:
            Set[str]: ì •ê·œí™”ëœ ìƒí’ˆ URL ì§‘í•©
        """
        urls = set()

        try:
            with open(self.csv_path, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    url = row.get('product_url', '').strip()
                    if url and url.startswith('http'):
                        normalized = normalize_url(url)
                        urls.add(normalized)

            self.logger.info(f"ğŸ“š ê¸°ì¡´ ìƒí’ˆ URL {len(urls)}ê°œ ë¡œë“œë¨")
            return urls

        except Exception as e:
            self.logger.warning(f"CSV ì½ê¸° ì‹¤íŒ¨: {e}")
            return set()

    def is_already_used(self, product_url: str) -> bool:
        """
        URLì´ ì´ë¯¸ ì‚¬ìš©ë˜ì—ˆëŠ”ì§€ í™•ì¸ (ì •ê·œí™”í•˜ì—¬ ë¹„êµ)
        
        Args:
            product_url: í™•ì¸í•  ìƒí’ˆ URL
        
        Returns:
            bool: ì´ë¯¸ ì‚¬ìš©ë˜ì—ˆìœ¼ë©´ True
        """
        normalized_input = normalize_url(product_url)
        all_urls = self.get_all_urls()

        is_used = normalized_input in all_urls

        if is_used:
            self.logger.info(f"ğŸ”„ ì¤‘ë³µ URL ë°œê²¬: {product_url}")

        return is_used

    def add_product(
        self,
        product_url: str,
        keyword: Optional[str] = None,
        product_name: Optional[str] = None
    ):
        """
        ìƒˆ ìƒí’ˆ ì¶”ê°€
        
        Args:
            product_url: ìƒí’ˆ URL
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
            product_name: ìƒí’ˆëª…
        """
        # ì¤‘ë³µ ì²´í¬
        if self.is_already_used(product_url):
            self.logger.warning(f"âš ï¸  ì´ë¯¸ ì¡´ì¬í•˜ëŠ” URL (ì¤‘ë³µ ë°©ì§€): {product_url}")
            return

        record = ProductRecord(
            product_url=product_url,
            keyword=keyword,
            created_at=datetime.now().isoformat(),
            product_name=product_name,
        )

        try:
            with open(self.csv_path, 'a', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=self.FIELDNAMES)
                writer.writerow(record.to_dict())

            self.logger.info(f"âœ… ìƒí’ˆ URL ì¶”ê°€ë¨: {product_url}")

        except Exception as e:
            self.logger.error(f"CSV ì“°ê¸° ì‹¤íŒ¨: {e}")

    def get_all_records(self) -> List[ProductRecord]:
        """ëª¨ë“  ë ˆì½”ë“œ ë°˜í™˜ (ë””ë²„ê¹…ìš©)"""
        records = []

        try:
            with open(self.csv_path, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    records.append(ProductRecord(
                        product_url=row.get('product_url', ''),
                        keyword=row.get('keyword'),
                        created_at=row.get('created_at'),
                        product_name=row.get('product_name'),
                    ))

            return records

        except Exception as e:
            self.logger.warning(f"CSV ì½ê¸° ì‹¤íŒ¨: {e}")
            return []

    def remove_duplicates(self):
        """ì¤‘ë³µ URL ì œê±° (ì •ê·œí™” ê¸°ì¤€)"""
        self.logger.info("ğŸ”§ ì¤‘ë³µ URL ì œê±° ì¤‘...")

        try:
            # ëª¨ë“  ë ˆì½”ë“œ ì½ê¸°
            records = self.get_all_records()

            # ì •ê·œí™”ëœ URL ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
            seen_urls = set()
            unique_records = []

            for record in records:
                normalized = normalize_url(record.product_url)
                if normalized not in seen_urls:
                    seen_urls.add(normalized)
                    unique_records.append(record)

            # íŒŒì¼ ë‹¤ì‹œ ì“°ê¸°
            with open(self.csv_path, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=self.FIELDNAMES)
                writer.writeheader()

                for record in unique_records:
                    writer.writerow(record.to_dict())

            removed_count = len(records) - len(unique_records)
            self.logger.info(f"âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: {removed_count}ê°œ ì œê±°, {len(unique_records)}ê°œ ë‚¨ìŒ")

        except Exception as e:
            self.logger.error(f"ì¤‘ë³µ ì œê±° ì‹¤íŒ¨: {e}")
